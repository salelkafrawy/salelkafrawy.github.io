---
layout: post
title: NHL Data Science Project - Milestone 2
---

{% comment %} 
########################################################################################### 
Q2 Feature Engineering 1 
###########################################################################################
 {% endcomment %}

**Contents**
* TOC
{:toc}

# Feature Engineering 1

## Tidied Dataset Visualization

Four additional columns were added to the data frame (three new features and the target label):
- `distance_from_net`: [`float`] the distance between the shot and the net
- `angle`: [`float`] the angle with respect to the net where the shot happened
- `is_empty_net`: [`bool`] the binary value that indicates whether the net was empty or not
- `is_goal`: [`bool`] the label column (a binary classification problem)

With the final objective of creating a learning model that can predict the probability of a shot to be a goal, these new features were plotted against different shot events to visualize and to gain insights into the dataset. The coordinate of the goal net was approximated with a single point as instructed. 

The graph below shows the relationship between shot events and the distance from the net. The resulting distribution shows that the shot counts were significantly lower outside the attacking/scoring zone. Within the attacking/scoring zone, there was a consistent number of shots distributed at all distances above ~15 ft. It appears that  lower distance to net values corresponds to higher number of goals. The count for both events peaked when the distance from net value was around 9 ft and decreased afterwards. 

![Shot Distribution by Distance from Net](/Images/M2_FE1_Q1_Shot_vs_Distance.png)

The graph below shows the relationship between shot events and the angle from net. A symmetrical distribution of the shot counts with respect to angle from net is observed. A higher distribution of shots is observed at both ~30 and ~-30 degrees. An abnormal number of shot counts is observed at 0 degree. Outside the 30 to -30 degrees angle range, the number of shot counts decreased as the angle increased.

![Shot Distribution by Angle from Net](/Images/M2_FE1_Q1_Shot_vs_Angle.png){: .center-image }

The graph below shows how shots were distributed over both distance and angle from the net. It is noted that the shape of the 2D histogram was governed by the boundary of the ice rink, where larger distance value would correspond to a smaller range of available angles. The graph reveals that the high shot counts region formed a trapezoidal shape distribution (Angle values between ~25 to ~-25 degrees when distance value is ~80 ft, and angle values between 35 and -35 when distance value is ~50 ft). This would correspond to a rectangular shaped high shot counts region within the attacking/scoring zone. It is also noted that there was a high distribution of shots at distance beyond the attacking/scoring zone. The angle and distance would suggest those shots occured in the defending zone.


![Shot Distribution by Distance and Angle from Net](/Images/M2_FE1_Q1_Shot_vs_Distance_Angle.png)


## Goal Rate Visualization

Plotting goal rate against distance from net and against angle from net reveals additional information on the probability of whether a shot would be a goal.
The graph below shows the relationship between goal rate and the shot distance from the net. The goal rate when the distance from net value is within ~80 ft appears to be reasonable, the probability of scoring was higher when the player was closer to the net. Utilizing information derived from the figures above and under the assumption that the data is correct, the higher goal rate distribution beyond ~80 ft can be attributed to the small sample size of shots from these distances.

![Goal Rate Distribution by Distance from Net](/Images/M2_FE1_Q2_GoalRate_vs_Distance.png)

The graph below shows the relationship between goal rate and the shot angle from net. The graph suggests that the highest goal rate occurred at angles between ~5 to ~10 and between ~-5 to ~-10 degrees. The higher the angle deviation from the center, the lower the goal rate. A dip is observed between ~5 and ~-5 degrees and an abnormal spike is observed at both 0 and ~90 degrees. Using the same assumptions and information as stated above, the anomaly observed at ~90 degrees can be attributed to the lower number of total shots from those angles. The anomaly  observed at 0 degree was the result of assuming the shots taken at zero distance had an angle of 0.

![Goal Rate Distribution by Angle from Net](/Images/M2_FE1_Q2_GoalRate_vs_Angle.png)



## Sanity Check Using Domain Knowledge

To score a non-empty net goal from the opposite side of the ice rink was incredibly rare. Therefore, any goals that occurred within the defensive zone could be due to mislabelling. The following figures show the relationship between goals and the shot distance from the net and include the information of whether the net was empty or not. 
By plotting the shot counts against distance larger than 80 ft (corresponds to distance outside the attacking/scoring zone), the plots reveal that there were many non-empty net goals, especially between ~160 to ~175 ft.

![Goal Distribution by Distance from Net](/Images/M2_FE1_Q3_Goal_vs_Distance.png)
![Goal Distribution by Distance above 80 ft from Net](/Images/M2_FE1_Q3_Goal_vs_DistanceGreaterThan80.png){: .center-image }

Further investigation revealed that the majority of goals returned by the following query had incorrectly-assigned coordinates

```python
shots[(shots.distance_from_net >= 150) & (shots.is_goal) & (~shots.is_empty_net)]
```

An example of such mislabelled data is Adam Cracknell's first period goal on February 21, 2016, against the Colorado Avalanche.

Reviewing video of the goal available on  [[nhl.com](https://www.nhl.com/video/cracknell-opens-the-scoring/t-278025682/c-41679503)] showed that the shot occurred at the opposite end of the ice from the location implied by the coordinates in the API record, i.e. just a few feet from the net.


![Sample of Mislabelled Data](/Images/M2_FE1_Q3_Mislabelled_Data.png)





{% comment %} 
########################################################################################### 
Q3 Baseline Models
###########################################################################################
 {% endcomment %}
# Baseline Models

## Logistic Regression Baseline
This section presents three logistic regression models: 
- `LR_distance_only`: trained on the `distance_from_net` feature only
- `LR_angle_only`: trained on the `angle` feature only
- `LR_distance_and_angle`: trained on both the `distance_from_net` and `angle` features

The training and validation data were generated by applying a random 80/20 split at the game level to all game data between 2015/16 - 2018/19. A default logistic regression classifier was implemented as instructed. The model achieved an accuracy of ~90 when tested using the validation data. The confusion matrix, shown below, reveals that the output of the three models was predominantly the "no-goal" label. The conclusion drawn from applying the cross validation method was that the predicted outcome is not affected by the shuffling of the data.

Such model behavior can be attributed to the overwhelmingly large quantity of "no-goal" label compared to the "goal" label at all distance values. In other words, the performance of the model was a result of the imbalanced dataset. Therefore, solely relying on the accuracy of the prediction is not an effective measurement of the model performance.


|                     | Predicted: No Goal | Predicted: Goal |
|:-------------------:|:------------------:|:---------------:|
| **Actual: No Goal** |        46131       |        0        |
|   **Actual: Goal**  |        4829        |        0        |


## Model Performance Comparison

Three logistic regression models and a random baseline model were created and the performance results plotted for comparison (See figure below). It was noted that all three logistic regression models achieved the same accuracy when tested using the validation data.

The receiver operating characteristic (ROC) curve plots the true positive rate of the prediction against the false positive rate and can be used as an alternative method to evaluate the accuracy of the model. A higher area under the curve (AUC) indicates the model is better at distinguishing between the two classes. Curves that lie above the baseline curve indicate the proportion of correctly classified labels is greater than the proportion of incorrectly classified labels. Therefore, the ROC curve suggests that the performance of the logistic regression model trained on distance from net is similar to the performance of the model trained on distance and angle from net. Both of these models outperformed the other two. Using only angle from net as input feature led to a performance comparable if not worse than the random baseline. The curves suggest that the distance from net feature is more important than the angle from net feature.

The goal rate (#goals/(#no_goals + #goals)) against shot probability model percentile is shown below titled "True Positive Rate by Percentile". The extremes (percentile above 97.5) were removed for clarity. An ideal model would have a high true positive rate at a high probability percentile. In other words, the probability of correct prediction should align with the goal rate. Similar to the conclusion drawn from the ROC curve, this graph suggests that using distance or distance and angle as input features led to a better performing model than using angle as input feature. In addition, the model trained using angle as input feature produced a lower true positive rate at high probability percentile than the random baseline model. This observation further reduces the value of using the angle feature as an input feature.

The curve titled "Positive Proportion by Percentile" plots the cumulative proportion against the estimated probability percentile. The ideal model would reach a cumulative proportion of goals equal to 1 at a high probability percentile, only limited by the data distribution. The shape of this curve appears similar to the ROC curve, in this instance a similar conclusion can be drawn as well.

The reliability graph shows the relationship between the predicted probability and the empirical probability. In other words, it correlates the model generated probabilities of a true label with the actual fraction of the true label. The curve for the ideal model would overlap the perfectly calibrated line. The curve of the random baseline is horizontal because the model generated probability is uniformly distributed. The curves for the models trained on distance only and the model trained on distance and angle are similar, while they do overlap the perfectly calibrated line, the predicted probabilities were concentrated and never exceed 0.2. The model trained on angle only generated a vertical curve which indicates the variance of the predicted probability is very small. This curve suggests that the model is incapable of correlating the angle feature to the label. As such, the angle feature did not aid the logistic regression model in increasing prediction accuracy and performance.


![Logistic regression models](/Images/m2_q3_logistic_regression_baselines.png)

*See Appendix for Comet.ml links*




{% comment %} 
########################################################################################### 
Q4 Feature Engineering 2 
###########################################################################################
 {% endcomment %}
# Feature Engineering 2
## New Features Description
The newly added features with a brief description are explained in the following format:
 <center> (feature name:[data type] description) </center>
- `period`: [`int`] the period of the game the event was recorded in .
- `distance_from_net`: [`float`] the distance between the shot and the net.
- `angle`: [`float`] the angle with respect to the net where the shot happened.
- `type`: [`str`] the shot type (i.e. the primary event type out of 25 types).
- `secondary_type`: [`str`] the secondary event type.
- `is_empty_net`: [`bool`] a binary feature indicates whether the net was empty or not.
- `coordinate_x`, `coordinate_y`: [`int`, `int`] two separate columns for the x and y coordinates of the event respectively.
- `game_sec`: [`int`] the total number of seconds elapsed in the game.

Using information from the immediately preceding event we added four new features:
- `prev_event_type`: [`str`] the preceding event's event type.
- `prev_event_x_coord`, `prev_event_y_coord`: [`int`, `int`] x and y coordinates for the previous event.
- `prev_event_time_diff`: [`int`] the time difference between the current and the preceding event in seconds.
- `distance_from_prev_event`: [`float`] the distance between the current event and the preceding one.

Additional features that quantify the state of the play are:
- `is_rebound`: [`bool`] a binary feature that is true if the current and preceding events were shots (i.e. rebound), otherwise false.
- `rebound_angle`: [`float`] only included if the shot is a rebound, otherwise 0. It is the angle between both shots.
- `speed`: [`float`] the speed of the puck in the current event and defined as: `distance_from_prev_event` \ `prev_event_time_diff`

Bonus features that capture power-play information:
- `friendly_skaters`: [`int`] the number of skaters on the ice for the shooter's team, value no less than 3.
- `opposing_skaters`: [`int`] the number of on-ice skaters for the opposing (non-shooter's) team, value no less than 3.
- `power_play_time_elapsed`: [`int`] power play timer for the shooter's team, only non-zero when `friendly_skaters` > `opposing_skaters`.


### Winnipeg vs Washington Dataframe
The saved data frame with the above features of the game id=2017021065 (Winnipeg vs Washington game on March 12, 2018) was stored in the dataframes folder under Assets & Artifacts in [this comet.ml experiment](https://www.comet.ml/tim-k-lee/ift6758-hockey/a6ab7af75c3f4c5282a122d71325dde1). 




{% comment %} 
########################################################################################### 
Q5 Advaneced Models
###########################################################################################
 {% endcomment %}
# Advanced Models

## XGBoost Baseline
In this section an XGBoost model was trained on both the `distance_from_net` and `angle` features, this model is referred to as the `xgboost_baseline` model in the upcoming sections.

The XGBoost library was utilized to create the gradient boosted tree classifier, an ensembling method that combines outputs from individual trees to generate the final prediction. A simple XGBoost model with a learning rate of 1, depth of 2, using logistic regression for binary classification was created. This baseline model used the same training/validation setup as the logistic regression models and was trained only on distance and angle features. Compared to the Logistic Regression model trained on distance and angle, the only difference is that the input features were not normalized. This decision was made based on the fact that decision trees are usually not affected by any monotonic transformation. 

The baseline XGBoost model showed a similar problem to its Logistic Regression counterpart: almost no non-zero predictions (i.e. goals) were made. Comparing the performance of the baseline XGBoost and Logistic Regression models in more detail using the classifier quality charts shows marginal improvement in performance by the XGBoost framework (See below). Most notably, the reliability curve shows that the XGBoost model produced predicted probabilities that are inline with the perfectly calibrated line at slightly higher values. 

|                     | Predicted: No Goal | Predicted: Goal |
|:-------------------:|:------------------:|:---------------:|
| **Actual: No Goal** |        60148       |        20        |
|   **Actual: Goal**  |        6203        |        19        |




![XGBoost Baseline](/Images/m2_q5_LR_vs_XGBoost.png)

*See Appendix for Comet.ml links*


## XGBoost Hyperparameter Tuning

The optimal hyperparameters were obtained through the random search cross validation method. A 5 fold cross-validation splitting strategy was selected, this strategy was repeated 50 iterations to determine the optimal combination. The combination was then refitted one last time using the entire dataset to avoid overfitting. The input features used in this experiment include the new features added in the previous sections but exclude the bonus features.

The parameters obtained through the optimization process are as follow:

|     <u>Hyperparameter</u>        | <u>Value</u> | 
|:-------------------|:------------------:|
| **Learning rate step size** [learning_rate]            | 0.1 | 
| **Number of gradient boosted trees** [n_estimators]    | 275 |  
| **Maximum tree depth** [max_depth]                     | 1 | 
| **Minimum loss reduction required for split** [gamma]  | 0.1 |

The identified optimal hyperparameters were used to create a tuned XGBoost classifier called `xgboost_optimal`. The confusion matrix shows that the tuned model predicted more goals than the baseline model and has a minor accuracy increment of ~2%. The two models were plotted on the evaluation figures as shown below. It is very apparent that the tuned model performed better than the baseline. It achieved an AUC of ~0.82, a ~15% increase from the baseline. Both the positive rate by percentile figure and the positive proportion by percentile figure suggest that the probability prediction of the tuned model is more aligned with the actual goal rate. This observation is further reinforced by the reliability curve. The model is capable of generating a high probability prediction even though most of those predictions lie above the perfectly calibrated line rather than overlapping it. Nevertheless, this is the best performing model to this point.

|                     | Predicted: No Goal | Predicted: Goal |
|:-------------------:|:------------------:|:---------------:|
| **Actual: No Goal**  |        60048       |        120        |
|   **Actual: Goal**  |        5110        |        1112        |


![XGBoost Hyperparameter Tuned](/Images/m2_q5_XGBoost_hyperparam_tuning.png)

*See Appendix for Comet.ml links*


## XGBoost Optimal Feature Set

Two methods of performing feature selection were explored, the Lasso and the SHAP explainer. 


### Lasso Feature Selection
The Lasso method selects features based on penalizing and reducing the corresponding coefficients. The coefficients for insignificant features will eventually be reduced to zero and thus dropped. The `sklearn.linear_model.Lasso` method (L1 regularization method) with a setting of `alpha=0.001` was used to penalize the feature coefficients of a trained linear model. The selected features by `Lasso` were (please note that feature names starting with `shot_` and `prev_event_` one-hot encoded features for `secondary_type` and `prev_event_type` features respectively):
- `coordinate_x`
- `coordinate_y`
- `distance_from_net`
- `angle`
- `angle_between_prev_event`
- `distance_from_prev_event`
- `prev_event_time_diff`
- `speed`
- `is_rebound`
- `rebound_angle`
- `is_empty_net`
- `shot_Backhand`
- `shot_Tip-In`
- `shot_Wrist Shot`
- `prev_event_FACEOFF`
- `prev_event_GIVEAWAY`
- `prev_event_HIT`
- `prev_event_SHOT`

A new XGBoost model called `xgboost_Lasso` was tuned (with the same hyperparameter search process described previously) with the previous set of features. The resulting hyperparameters are:

|     <u>Hyperparameter</u>        | <u>Value</u> | 
|:-------------------|:------------------:|
| **Learning rate step size** [learning_rate]            | 0.062 | 
| **Number of gradient boosted trees** [n_estimators]    | 495 |  
| **Maximum tree depth** [max_depth]                     | 5 | 
| **Minimum loss reduction required for split** [gamma]  | 0.184 |

### SHAP Explainer as Feature Selector
The SHAP method calculates the impact of each feature to the model's prediction and selects features with the highest importance towards that prediction. After inspecting several data point explanations with SHAP, we selected the following list of features that we found the most important features in those explanations according to SHAP. These features are:  
 - `distance_from_net`
 - `is_rebound`
 - `prev_event_SHOT`
 - `prev_event_time_diff`
 - `angle`
 - `is_empty_net`
 - `shot_Snap Shot`
 - `shot_Slap Shot`
 - `distance_from_prev_event`
 - `coordinate_y`
 - `prev_event_HIT`

The already tuned hyperparameters for this `xgboost_SHAP` model was tuned again using the same optimization process but with the new feature set. The resulting hyperparameters are:

|     <u>Hyperparameter</u>        | <u>Value</u> | 
|:-------------------|:------------------:|
| **Learning rate step size** [learning_rate]            | 0.062 | 
| **Number of gradient boosted trees** [n_estimators]    | 495 |  
| **Maximum tree depth** [max_depth]                     | 5 | 
| **Minimum loss reduction required for split** [gamma]  | 0.184 |


The results of the tuned models with the two feature selection methods were plotted on the classifier quality charts with the previous XGBoost models (i.e. `xgboost_baseline`). The ROC, positive rate by percentile, and positive proportion by percentile all show a comparable result between the hyperparameter optimized model and the models that underwent feature selection. Improvement in performance is observed in the reliability figure. The tuned models trained on the selected features are much closer to the perfectly calibrated line than all other models. The `xgboost_SHAP` was selected to be the best performing XGBoost model due to the lower number of features used.  

![XGBoost Feature Selected](/Images/m2_q5_xgboost+feat_sel.png)

*See Appendix for Comet.ml links*




{% comment %} 
########################################################################################### 
Q6 Our Best Shot
###########################################################################################
 {% endcomment %}
# Our Best Shot!

## Neural Networks Models
Neural networks was one of the techniques explored to create a better goal probability prediction model. Four different Multi-Layer Perceptron (MLP) models were created using different input feature sets. The name of the models and the features used are:
- `NN_distance`: trained on `distance_from_net` feature
- `NN_baseline`: trained on both `distance_from_net` and `angle` features
- `NN_basic`: trained on `period`, `goals_home`, `goals_away`, `shooter_id`, `coordinate_x`, `coordinate_y`, `distance_from_net` and `angle` features
- `NN_adv`: trained on `game_sec`, `period`, `secondary_type`, `coordinate_x`, `coordinate_y`, `distance_from_net`, `angle`, `prev_event_type`, `angle_between_prev_event`, `distance_from_prev_event`, `prev_event_time_diff`, `speed`, `is_rebound`, `rebound_angle` and `is_empty_net` features

All the feature set were preprocessed using the following procedure:
- Consolidate the representation of certain categorical features by relabeling different events with the same label.
- Encode categorical features using one-hot encoding.
- Normalize all features by subtracting the feature mean and scaling to unit variance.
- Replace all NaN values with 0.
   
The MLP models used the Adam optimization algorithm with a logistic activation function and adaptive learning rate. The number of neurons in the input layer was selected to be the number of input features plus one, similarly the output layer had one output neuron for the binary class plus one additional neuron. The one extra neuron was intended to represent the bias term. Only a single hidden layer was used, the number of hidden neurons was the mean of the input neurons and the output neurons.
 
The MLP models trained on the distance, baseline, and basic feature set produced the same accuracy of ~90 % and the same confusion matrix with zero prediction of goals. The MLP model trained on the advance feature set produced an accuracy of ~92 % and the prediction of goals was non-zero as shown in the confusion matrix below.


|                     | Predicted: No Goal | Predicted: Goal |
|:-------------------:|:------------------:|:---------------:|
| **Actual: No Goal** |        46077       |        55       |
|   **Actual: Goal**  |        3929        |        900     |

Evaluating the models using the classifier quality charts suggests that the performance of the model increases as the number of input features increases as shown below. The model trained on the advance feature set outperformed all other models. The increase in AUC using the distance feature set and the advance feature set was close to 20 %. The reliability curve shows that using the advance feature set allowed the model to become more aligned with the perfectly calibrated line. The true positive rate curve shows a significant performance improvement at high probability percentiles when using the advance feature set.
 
![Neural Network Models Comparison](/Images/m2_q6_nn_models.png)
*See Appendix for Comet.ml links*

The effect of the number of hidden layer neurons was investigated and the results plotted in the evaluation figures shown below. The model trained on the advance feature set was used in this exercise. The figures suggest that the number of hidden neurons had no direct impact on the performance of the model.

![Neural Network Models Comparison](/Images/M2_BS_Q1_HiddenNeuronComparison.png)


## Advanced Feature Selection Method
All features generated in the advanced feature engineering section were inspected to determine the correlation between feature pairs. A high correlation value indicates that the two features would provide similar information to the model. As a result, one of the redundant features in the feature pair was removed.

The following figure shows the correlation between all input features.
![Features Correlation](/Images/m2_q6_features_correlation.png)

As shown in the figure, a high correlation can be observed between the `is_rebound` feature and two other features: (`rebound_angle` and `prev_event_SHOT`). The figure also shows that the `period` feature was highly correlated with `game_sec`. And both `coordinate_x` and `coordinate_y` features are correlated with `distance_from_net` and `angle` features respectively. 

After removing the following features (`is_rebound`, `coordinate_x`, `coordinate_y`, `period`), an XGBoost model was trained using the remaining features. This model was named `xgboost_feats_non_corr`, a logistic regression model, named `lr_non_corr_feats`, was also trained using the same features for comparison. The performance of these models are shown and discussed in the model comparison and selection section. 

The hyperparameter for the tuned XGBoost model (using the same optimization process used in previous sections) were:

|     <u>Hyperparameter</u>        | <u>Value</u> | 
|:-------------------|:------------------:|
| **Learning rate step size** [learning_rate]            | 0.062 | 
| **Number of gradient boosted trees** [n_estimators]    | 495 |  
| **Maximum tree depth** [max_depth]                     | 5 | 
| **Minimum loss reduction required for split** [gamma]  | 0.184 |

The following figure shows the correlation between the remaining features:
![Features Correlation](/Images/m2_q6_features_non_correlated.png)
*See Appendix for Comet.ml links*

## Handling Imbalanced Data
The Synthetic Minority Oversampling Technique (SMOTE) was employed to address the imbalanced dataset issue. Unlike the bootstrapping method, this method creates new synthetic data points that are different from the existing data. This is achieved by drawing lines between existing data points for the minority class and creating new data points that lie on the connection lines. In other words, this method populates the area within the boundary defined by the existing data points. 

A logistic regression model and a XGBoost model were used to evaluate the effect of the SMOTE method. The feature set that underwent oversampling was the non-highly-correlated feature set established above. The two models trained on the resulting oversampled data were named `lr_SMOTE` and `xgboost_SMOTE`.

The prediction results on the validation set made by the two models were plotted on the classifier quality chart for comparative performance evaluation. Other than the reliability chart, the remaining charts show that both models were able to achieve a high AUC score and had a comparable performance. The reliability chart shows that the `xgboost_SMOTE` model to be the superior model as it was more aligned with the perfectly calibrated line.

![SMOTE Performance](/Images/m2_q6_SMOTE_performance.png)
*See Appendix for Comet.ml links*


## Model Comparison and Selection
Seven models discussed above were compared using the classifier quality charts. These models are:
- `xgboost_lasso`: XGBoost model trained on selected features by Lasso method
- `NN_adv`: Neural network model trained on the advance feature set
- `xgboost_feats_non_corr`: XGBoost model trained on all the non-highly-correlated features
- `xgboost_SMOTE`: XGBoost model trained on all the non-highly-correlated features over sampled using the SMOTE method
- `lr_all_feats`: Logistic regression model trained on all the generated features from Feature Engineering 2 part
- `lr_non_corr_feats`: Logistic regression model trained on the non-highly correlated features
- `lr_SMOTE`: Logistic regression model trained on the non-highly correlated features oversampled using the SMOTE method

As shown in the following figures, all models achieved very similar performance when evaluated using the positive rate by percentile chart and the positive proportion by percentile chart. The ROC chart shows that the `xgboost_feats_non_corr` model had the highest AUC score (0.83) and the `lr_non_corr_feats` had the lowest (0.81). The variation between AUC scores were too low to be conclusive. The reliability chart offers a clearer distinction and shows that the top performing models are `NN_adv`,`xgboost_feats_non_corr`, and `xgboost_lasso`. The performance curve produced by these models are closest to the perfectly calibrated line. The graph also shows that `lr_all_feats` and `lr_non_corr_feats` achieved a similar performance. Bith `xgboost_SMOTE` and `lr_SMOTE` did not perform as well as the other models, their performance curves had the largest deviation from the perfectly calibrated line. Oversampling did not lead to better performance and may have overfitted the models. In conclusion, the charts suggest that the `xgboost_feats_non_corr` to be the best performing model. 

![Best Performers](/Images/m2_q6_best_performers.png)


{% comment %} 
########################################################################################### 
Q7 Evaluate on Test set
###########################################################################################
 {% endcomment %}
# Evaluate on Test Set

## Model Evaluation For the 2019/20 Regular Season Games

All models were retrieved from comet.ml and used to predict the test set's probability of goals. The test set was divided and grouped by regular season games and postseason games. Two separate classifier quality charts were generated as a result.

The prediction accuracy for `lr_SMOTE` and `lr_non_corr_feats` both decreased significantly when tested on the test set, this indicates that both `lr_SMOTE` and `lr_non_corr_feats` are overfitted.

The test set prediction accuracy and AUC value for the regular season games matched results obtained from the validation set for the remaining models. As shown below, the classifier quality charts confirm that there was no significant variation in model performance. Information from the ROC curve and the reliability curve suggest that the top three performing models are `NN_adv`, `xgboost_SHAP`, and `xgboost_feats_non_corr`. The worst performing models are `lr_SMOTE` and `lr_non_corr_feats`, the two overfitted models. Based on the results observed using the test set, it can be concluded that the validation set is a good representation of the general data.


|   <u>Model Name</u>          | <u>Accuracy</u> | <u>AUC</u> |
|:-------------------|:------------------:|:---------------:|
| **LR_angle_only**             |       90.3     |      0.51     |
| **LR_distance_only**          |       90.3     |      0.70     |
| **LR_distance_and_angle**     |       90.3     |      0.70     |
| **xgboost_lasso**             |       91.9     |      0.84     |
| **xgboost_SHAP**              |       92.0     |      0.83     |
| **xgboost_feats_non_corr**    |       92.0     |      0.84     |
| **NN_adv**                    |       92.0     |      0.84     |
| **xgboost_SMOTE**             |       91.5     |      0.83     |
| **lr_SMOTE**                  |       86.0     |      0.29     |
| **lr_all_feats**              |       91.5     |      0.83     |
| **lr_non_corr_feats**         |       56.9     |      0.50     |


<!-- |                                        | <u>Accuracy</u> | <u>AUC</u> |
|:-------------------|:------------------:|:---------------:|
| **Logistic Regression Distance Only**      |       90       |      0.70     |
| **Logistic Regression Angle Only**         |       90       |      0.51     |
| **Logistic Regression Distance and Angle** |       90       |      0.70     |
| **XGBoost Model with SHAP**                |       92       |      0.83     |
| **XGBoost Model with Non Correlated Features**      |       92       |      0.84     | -->


![Regular Season Test Set](/Images/m2-q7-regular-games.png)

## Model Evaluation For the 2019/20 Post Season Games 

The model performance results evaluated using the postseason games were similar to the results obtained through the regular season games. The table below shows that there was only a minor increase in prediction accuracy and the AUC values remained the same. The curves in the classifier quality charts generated using the postseason game predictions appear to be noisier than the curves in the previous charts. This observed granularity is attributed to the lower number of games in the dataset. One noticeable difference can be observed between the regular season and postseason reliability charts. While the general shape of the curves remained similar, most of the models were unable to produce probabilities above 0.6. Only `xgboost_SMOTE`, `lr_SMOTE`, and `lr_non_corr_feats` produced probability predictions above 0.6. While the difference observed here can be attributed to the lower number of samples available, it also shows that the postseason games dataset has different traits compare to the regular season games dataset. `xgboost_lasso`, `NN_adv`, `xgboost_SHAP` and `xgboost_feats_non_corr` remined as the top three performing models when evaluated using the postseason dataset.


|   <u>Model Name</u>          | <u>Accuracy</u> | <u>AUC</u> |
|:-------------------|:------------------:|:---------------:|
| **LR_angle_only**             |       91.2     |      0.51     |
| **LR_distance_only**          |       91.2     |      0.71     |
| **LR_distance_and_angle**     |       91.2     |      0.71     |
| **xgboost_lasso**             |       92.9     |      0.84     |
| **xgboost_SHAP**              |       93.0     |      0.83     |
| **xgboost_feats_non_corr**    |       92.8     |      0.84     |
| **NN_adv**                    |       92.9     |      0.84     |
| **xgboost_SMOTE**             |       92.6     |      0.83     |
| **lr_SMOTE**                  |       86.4     |      0.30     |
| **lr_all_feats**              |       92.3     |      0.83     |
| **lr_non_corr_feats**         |       59.1     |      0.49     |


<!-- |                                        | <u>Accuracy</u> | <u>AUC</u> |
|:-------------------|:------------------:|:---------------:|
| **Logistic Regression Distance Only**      |       91       |      0.71     |
| **Logistic Regression Angle Only**         |       91       |      0.51     |
| **Logistic Regression Distance and Angle** |       91       |      0.71     |
| **XGBoost Model with SHAP**                |       93       |      0.83     |
| **XGBoost Model with Non Correlated Features**      |       93       |      0.84     | -->


![Post Season Test Set](/Images/m2-q7-post-games.png)

# Appendix - Comet.ml Links

## Baseline Experiments
- **[`LR_distance_only`](https://www.comet.ml/tim-k-lee/ift6758-hockey/f3c8d785b3ca4ac480b877d4c4199c3c)**: Logistic regression model trained on the `distance_from_net` feature only
- **[`LR_angle_only`](https://www.comet.ml/tim-k-lee/ift6758-hockey/370c0e0b473b44babbc162ccd1d5aa16)**: Logistic regression model trained on the `angle` feature only
- **[`LR_distance_and_angle`](https://www.comet.ml/tim-k-lee/ift6758-hockey/dcea9b13ba6642f090d65b684b7e9d14)**: Logistic regression model trained on both the `distance_from_net` and `angle` features

## Advanced Experiments
- **[`xgboost_baseline`](https://www.comet.ml/tim-k-lee/ift6758-hockey/6674103c7b7a4a38bcce89ca60d69cd6)**: XGBoost model trained on the `angle` and `distance_from_net` features only
- **[`xgboost_optimal`](https://www.comet.ml/tim-k-lee/ift6758-hockey/0bf852d52c004156abf54cc2ef5e297d)**: XGBoost model that was tuned and optimal hyperparameters were chosen
- **[`xgboost_lasso`](https://www.comet.ml/tim-k-lee/ift6758-hockey/98ccf2eda8944da6b94e8eac3fb56769)**: XGBoost model trained on selected features by Lasso method
- **[`xgboost_SHAP`](https://www.comet.ml/tim-k-lee/ift6758-hockey/360783a9e6294503a9290b1b36e263b0)**: XGBoost model trained on selected features by SHAP method


## Our Best Shot Experiments!
- **[`lr_all_feats`](https://www.comet.ml/tim-k-lee/ift6758-hockey/4f67f15d5fec4b39b1efae8e31e24c72)**: Logistic regression model trained on all the generated features from Feature Engineering 2 part
- **[`lr_non_corr_feats`](https://www.comet.ml/tim-k-lee/ift6758-hockey/c0f87c0971fd4cc39431b5e5d53347c9)**: Logistic regression model trained on the non-highly correlated features
- **[`lr_SMOTE`](https://www.comet.ml/tim-k-lee/ift6758-hockey/7aaa91b314c945fdb12b3103baaf81ac)**: Logistic regression model trained on the non-highly correlated features oversampled using the SMOTE method
- **[`xgboost_SMOTE`](https://www.comet.ml/tim-k-lee/ift6758-hockey/04b819114d484137a202b13cba10ce45)**: XGBoost model trained on the non-highly correlated features oversampled using the SMOTE method
- **[`xgboost_lasso`](https://www.comet.ml/tim-k-lee/ift6758-hockey/98ccf2eda8944da6b94e8eac3fb56769)**: XGBoost model that was trained on selected features by Lasso method
- **[`NN_adv`](https://www.comet.ml/tim-k-lee/ift6758-hockey/28f4a2c65ff8481f8bad834927a7513a)**: Neural network model trained on the advance feature set
- **[`xgboost_feats_non_corr`](https://www.comet.ml/tim-k-lee/ift6758-hockey/45fcfdff52d842e097135197995e985a)** XGBoost model that was trained on all the non-highly-correlated features







